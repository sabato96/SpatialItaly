---
title: "Progetto Markdown"
runtime: shiny
output: 
  html_document:
     theme: journal
     code_folding: hide
     number_sections: true
     toc: true
     toc_float: true
     toc_depth: 2
     includes:
       in_header: header.Rhtml
       after_body: footer.Rhtml
bibliography: biblio.bib
link-citations: yes
---
<style>
body .main-container {
max-width: 1350px;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
load("C:/Users/gargi/Desktop/Progetto Lab/.RData")


# Librerie
#####
library(plyr)
library(tidyverse)
library(ggmap)
library(sp)
library(sf)
library(tmap)
library(tmaptools)
library(leaflet)
library(RColorBrewer)
library(shinyjs)
library(spdep)
library(dplyr)
library(shiny)
library(shinydashboard)
library(DT)
library(ggplot2)
library(plotly)


ggmap::register_google(key="AIzaSyCCCEngniuORKmDvGus4ppsJ7oksOjKrWU")

```

# Introduzione
Il divario economico tra Nord e Sud Italia è fin dal 1861 un problema sociale ma soprattutto economico che si ripercuote sulla qualità della vita di milioni di persone. Secondo i dati dell'ISTAT il Mezzogiorno presenta un ampio differenziale di crescita rispetto al Nord. Nel corso degli anni molti studi hanno provato a identificare le cause di questo divario, che rimangono tutt'oggi difficili da individuare con precisione. Ciò che possiamo sicuramente asserire è che una delle conseguenze di questo divario è l'assenza, al Sud, di un tessuto industriale strutturato e di conseguenza il mercato del lavoro è profondamente diverso da quello del resto d'Italia, condannando i giovani, in particolare i meridionali, ad un futuro incerto e precario dal punto di vista lavorativo. 


Questo lavoro si pone l’obiettivo di studiare la distribuzione spaziale delle varie tipologie di reddito sul territorio italiano, in particolare verranno utilizzati metodi propri dell’analisi spaziale al fine di valutare se ci sono zone dello stivale in cui le variabili osservate presentano autocorrelazione spaziale o meno.

Lo studio procederà per gradi. In primis verranno analizzati i dati dal punto di vista descrittivo per identificare le propietà statistiche delle variabili considerate e quindi procedere ad una trattazione efficace. Successivamente verranno gettate le basi teoriche dell'analisi spaziale descrivendo minuziosamente il framework utilizzato per l'analisi.


# Data Exploration
## Data Wrangling

I dati sui redditi per comune sono quelli forniti dall’ISTAT relativi all’anno 2016. Il problema principale da affrontare nella prima fase del lavoro è stato quello di valutare la qualità e la coerenza dei dati forniti dall’ Istituto. In particolare nel dataset oggetto di studio erano presenti comuni conteggiati più volte. Una volta individuati questi comuni è stato possibile rimuovere i doppioni e procedere alla fase seguente. 
  
Per ogni tipologia di reddito rilevata erano presenti due variabili, cioè la frequenza delle osservazioni e l’ammontare totale rilevato; Al fine di rendere più semplice la trattazione è stata calcolata una media per ogni comune dividendo l’ammontare totale per la frequenza per tutte le tipologie di reddito rilevate. Questo ci ha permesso di ridurre il numero di variabili osservate. Dopo questa operazione preliminare il dataset presentava la struttura seguente:       

<br><br>

```{r}

shinyApp(
        
  ui <- fluidPage(
                 
                  DTOutput("table", width = "100%", height = 600)
                  
                       ),

  server <- function(input,output) {
          
      
    output$table <- DT::renderDataTable({ 
      
    
      a <- red.fin %>% mutate_if(is.numeric, ~round(.,3))
      
      datatable(a[,-4], filter = "top", options = list(scrollX = TRUE, target = "column") ) 
      
      
      }) 
    
   
    
    } ,
  
  options = list(height = 700)
  
)



```

<br><br>

## Descriptive statistics
Una volta che il dataset è stato corretto e sintetizzato nel modo sopra descritto diventa possibile calcolare tutte le statistiche descrittive necessarie per analizzare quantitativamente le variabili di interesse. 

<br><br>
```{r}

shinyApp(
        
ui <- fluidPage(

  sidebarLayout(

    # Sidebar panel per input ----
    sidebarPanel(
      
                        selectInput(inputId = "red2",
                              label = "Choose a variable to display",
                              choices = etichette[1:7],
                              selected = ""),
                        
                        
                        selectInput(inputId = "palette3",
                              label = "Choose color palette",
                              choices = list("Blues","BuGn",
                                             "Greens","Greys",
                                             "PuBuGn","Dark2"),
                              selected = "Greys"),
                        
      
      br(),

      
                        actionButton("Run2", "Run code")


    ),

    
    mainPanel(

      # Output: Tabset w/ plot, summary, and table ----
      tabsetPanel(type = "tabs",
                  tabPanel("Circ. plot", plotOutput("cirplot", height = 600)),
                  tabPanel("Viol. plot", plotlyOutput("violinplot", height = 600))
                  )
                  

    )
  )
),
  


server = function(input,output) {
          
  output$violinplot <- renderPlotly({
      
      
    if(input$Run2 == 0)
      return()

    isolate({
 
    
      xx <- input$red2
      mean <- mean(red.fin[[xx]], na.rm = TRUE)
      sd <- sd(red.fin[[xx]], na.rm = TRUE)

      yy <- red.fin[etichette]
      
      library(data.table)
      outlierReplace = function(dataframe, cols, rows, newValue = NA) {
        if (any(rows)) {
          set(dataframe, rows, cols, newValue)
        }
      }
      
      outlierReplace(yy, xx, which(yy[[xx]] > max(yy[[xx]], na.rm = TRUE)-3000))
      

      yy$macro <- red.fin$macro
      yy <- yy[-1,]
      
      #yy[sapply(yy, is.null)] <- NA
      
      y <- cbind(yy$macro,yy[xx])
      names(y)[1] <- "macro"
      
      p <- ggplot(y, aes(y=y[[xx]], x=y$macro, fill = y$macro)) + 
        geom_violin() + 
        theme(legend.title=element_text(face="bold")) + xlab("Aree geografiche") +
        labs(fill="Aree geografiche") + theme(legend.text = element_text( face = "bold")) +
        scale_fill_brewer(palette=input$palette3)
      
      p 

   # p <- ggplot(red.fin, aes(y=red.fin[[xx]], x=red.fin$macro, fill=red.fin$macro)) + 
   #      geom_violin(trim = TRUE) + 
   #      theme(legend.title=element_text(face="bold")) + xlab("Aree geografiche") +
   #      labs(fill="Aree geografiche") + theme(legend.text = element_text( face = "bold")) +
   #      scale_fill_brewer(palette=input$palette3)
    
   p #+ scale_y_continuous(name = xx, limits = c(mean-3*sd,mean+3*sd))
   
     
      })
      
      
    })
      
  output$cirplot <- renderPlot({
    
    
    if(input$Run2 == 0)
      return()
    isolate({
    
    data <- mean.reg[,c(2,24,3:9)]
    
    et <- input$red2
    
    dat. <- data.frame(
      individual = data$Regione,
      group = data$macro,
      value = data[et]
    )
    
    names(dat.) <- c("individual","group","value")
    dat. = dat. %>% arrange(group,value)
    
    
    
    # empty bar è una barra vuota che aggiunge spazio tra ogni gruppo
    empty_bar <- 4
    to_add <- data.frame( matrix(NA, empty_bar*nlevels(dat.$group), ncol(dat.)) )
    colnames(to_add) <- colnames(dat.)
    to_add$group <- rep(levels(dat.$group), each=empty_bar)
    dat. <- rbind(dat., to_add)
    dat. <- dat. %>% arrange(group)
    dat.$id <- seq(1, nrow(dat.))
    
    # Ottieni nome e la posizione y di ogni etichetta
    label_data <- dat.
    number_of_bar <- nrow(label_data)
    angle <- 90 - 360 * (label_data$id-0.5) /number_of_bar     # I substract 0.5 because the letter must have the angle of the center of the bars. Not extreme right(1) or extreme left (0)
    label_data$hjust <- ifelse(angle < -90, 1, 0)
    label_data$angle <- ifelse(angle < -90, angle+180, angle)
    
    # prepare a data frame for base lines
    base_data <- dat. %>% 
      group_by(group) %>% 
      summarize(start=min(id), end=max(id) - empty_bar) %>% 
      rowwise() %>% 
      mutate(title=mean(c(start, end)))
    
    # prepare a data frame for grid (scales)
    grid_data <- base_data
    grid_data$end <- grid_data$end[ c( nrow(grid_data), 1:nrow(grid_data)-1)] + 1
    grid_data$start <- grid_data$start - 1
    grid_data <- grid_data[-1,]
    
    
    mea <- mean(dat.$value, na.rm = TRUE)
    sd <- sd(dat.$value, na.rm = TRUE)
    
    lev <- c(mea-3*sd, mea-2.3*sd, mea-1.5*sd, mea+0.5*sd)
    
    
    # Make the plot
    p <- ggplot(dat., aes(x=id, y=value, fill=group)) +       # Note that id is a factor. If x is numeric, there is some space between the first bar
      geom_bar(aes(x = as.factor(id), y = value, fill = group), stat = "identity", alpha = 0.7) +
      
      geom_segment(data=grid_data, aes(x = end, y = lev[4], xend = start, yend = lev[4]), colour = "black", alpha=1, size=0.8 , inherit.aes = FALSE ) +
      geom_segment(data=grid_data, aes(x = end, y = lev[3], xend = start, yend = lev[3]), colour = "black", alpha=1, size=0.6 , inherit.aes = FALSE ) +
      geom_segment(data=grid_data, aes(x = end, y = lev[2], xend = start, yend = lev[2]), colour = "black", alpha=1, size=0.3 , inherit.aes = FALSE ) +
      geom_segment(data=grid_data, aes(x = end, y = lev[1], xend = start, yend = lev[1]), colour = "black", alpha=1, size=0.3 , inherit.aes = FALSE ) +
      
      annotate("text", x = rep(max(dat.$id),4), y = signif(lev,digits=6), label = as.character(signif(lev),digits=6) , color="grey", size=5 , angle=0, fontface="bold", hjust=1) +
      
      geom_bar(aes(x=as.factor(id), y=value, fill=group), stat="identity", alpha=0.6) +
      
      
      ylim(-1500,max(data[et])+2200) +
      theme_minimal() +
      theme(
        legend.position = "none",
        axis.text = element_blank(),
        axis.title = element_blank(),
        panel.grid = element_blank(),
        plot.margin = unit(rep(-1,9), "cm") 
      ) +
      coord_polar() + 
      geom_text(data=label_data, aes(x=id, y=value+60, label=individual, hjust=hjust), 
                color="black", fontface="bold",alpha=0.9, size=5, angle= label_data$angle, inherit.aes = FALSE ) + 
      
      geom_segment(data=base_data, aes(x = start, y = -5, xend = end, yend = -5), colour = "black", alpha=0.8, size=0.6 , inherit.aes = FALSE )  
    
    p
    
    })
    
    
    
  })


    } ,
  
  options = list(height = 800)
  
)

```



# Spatial analysis

## Spatial Objects

Per procedere all’analisi spaziale vera e propria bisogna definire un framework per posizionare nello spazio le unità amministrative oggetto dell’analisi. Al fine di definire nel modo più accurato possibile le unità spaziali dei comuni e delle varie regioni amministrative italiane sono stati utilizzati gli shapefile presenti sul sito [ISTAT](https://www.istat.it/it/archivio/222527) con riguardo a quelli dell’anno 2016, che corrisponde all’anno di censimento del dataset. 

Queste strutture dati sono in formato WGS84, facilmente caricabili in R grazie alla funzione readOGR presente nel pacchetto rgdal.
In questo modo abbiamo ottenuto i poligoni di regioni, province e comuni italiani. Qui un piccolo esempio.

<br><br>
```{r image_grobs, fig.show="hold", fig.align="default", out.width="50%"}


plot(OGR.reg, col = "gray", border = "blue") # Disegna shapefile vuoto 
plot(OGR.prov, col = "gray", border = "blue") # Disegna shapefile vuoto 

```

<br>
Nel caso sopra riportato  i poligoni rappresentano la forma e la superficie di regioni e province italiane. Utilizzando la stessa procedura e i dati forniti dall’ISTAT sono stati importati in R anche gli shapefile per i comuni.

Una volta importati gli shapefile per comuni, province e regioni è stato possibile inserire i dati utilizzando la funzione merge di R prendendo come riferimento la denominazione delle unità amministrative. In questo modo l’oggetto SpatialPolygonsDataFrame (S4 class) conterrà sia le informazioni spaziali sia i dati sui redditi per ogni poligono considerato. E’ necessario precisare, infine, che nel caso di regioni e province i dati comunali sono stati raggruppati per provincia o regione. Il valore del reddito per provincia (regione) sarà quindi uguale alla media del reddito dei comuni comuni presenti nella provincia (regione) . 

Nell'app seguente è possibile osservare i dati per provincia (leaflet global) e quelli per comune (leaflet local). Per quanto riguarda la visualizzazione dei comuni, è possibile scegliere quale macroregione italiana visualizzare, questo poichè il caricamento dei comuni su tutto il territorio nazionale sarebbe stato computazionalmente oneroso e avrebbe richiesto molto tempo per il rendering.

<br><br>
```{r}

shinyApp(
        
  ui <- dashboardPage(
        dashboardHeader(title = "Leaflet map"),
          dashboardSidebar(
             sidebarMenu(
              
               menuItem("Leaflet map", tabName = "leaflethome", icon = icon("dashboard"),
               menuSubItem("Leaflet global", tabName = "leafletglobal", icon = icon("dashboard")), #4
               menuSubItem("Leaflet local", tabName = "leafletlocal", icon = icon("dashboard")) #9
               ))), 
          
          
        dashboardBody(
          tabItems(
              
      
      tabItem(tabName = "leafletglobal",
          
#####           

fluidPage(
  
  #sidebar panel serve a inserire le scelte dell'utente
  
  
  fluidRow(
    column(3,
           
           selectInput(inputId = "red4",
                       label = "Choose a variable to display",
                       choices = etichette,
                       selected = "")),
    
    column(3,
           numericInput(inputId = "g.col1",
                        label = ("Choose n colors"),
                        value = "5")),
    column(3,
           
           selectInput(inputId = "palette4",
                       label = "Choose color palette",
                       choices = list("Blues","BuGn",
                                      "Greens","Greys",
                                      "PuBuGn","Dark2"),
                       selected = "Greys")),
    
    column(3,actionButton("Run4", "Run code")),
    
    
    leafletOutput("qtm", width = "100%", height = 800),
  
    
  ))),
#####  

      tabItem(tabName = "leafletlocal",
#####           
fluidPage(
  
  #sidebar panel serve a inserire le scelte dell'utente
  
  
  fluidRow(
    column(2,
           
           selectInput(inputId = "shape1",
                       label = "Choose a macro to display",
                       choices = list("Nord",
                                      "Centro",
                                      "Sud"),
                       selected = "Nord")),
    
    column(2,
           selectInput(inputId = "red9",
                       label = "Choose a variable to display",
                       choices = etichette,
                       selected = "")),
    column(2,
           
           numericInput(inputId = "g.col2",
                        label = ("Choose n colors"),
                        value = "5")),
    
    column(2,
           selectInput(inputId = "palette9",
                       label = "Choose color palette",
                       choices = list("Blues","BuGn",
                                      "Greens","Greys",
                                      "PuBuGn","Dark2"),
                       selected = "Greys")),
    
    column(2,
           checkboxInput("all1","All")),
    
    column(2,
           actionButton("Run9", "Run code")),
           
    
    
    
    leafletOutput("qtm.local", width = "100%", height = 800),
    
    
    
    
    
    
    
    
  )))))),
#####  

                  
        
server <- function(input,output) {
          
  
 
  output$qtm <- renderLeaflet({
    
    if(input$Run4 == 0)
      return()
    
    isolate({
      
      x <- input$g.col1
      y <- input$red4
      tm <- tm_shape(OGR.prov) + tm_fill(input$red4, palette = input$palette4, style = "quantile", 
                                         n = x , contrast = c(0.28, 0.87),
                                         id = "DEN_PCM") + 
        
            tm_borders(alpha=.7) + tm_legend(legend.position = c("left", "bottom")) +
            tm_layout(title = paste(y,"medio per provincia"),
                  title.size = 1.1) +
        
            tm_shape(OGR.reg) + tm_borders(col = "black") 
      
      tmap_mode("view")
      
      tmap_leaflet(tm) 
      
    }) })
  
  
  output$qtm.local <- renderLeaflet({
    
    if(input$Run9 == 0)
      return()
    
    isolate({
      
      x <- input$g.col2
      y <- input$red9
      
      if(input$shape1 == "Nord" && input$all1==FALSE){
        
        tm <- tm_shape(OGR.com.nord) + tm_fill(input$red9, palette = input$palette9, style = "quantile", 
                                               n = input$g.col2, contrast = c(0.28, 0.87),
                                               id = "COMUNE") + 
          
          tm_borders(alpha=.7) + tm_legend(legend.position = c("left", "bottom")) +
          tm_layout(title = paste(y,"medio per comune"),
                    title.size = 1.1) +
          
          tm_shape(OGR.reg) + tm_borders(col = "black") 
      }
      
      if(input$shape1 == "Centro" && input$all1==FALSE){
        
        tm <- tm_shape(OGR.com.centro) + tm_fill(input$red9, palette = input$palette9, style = "quantile", 
                                                 n = input$g.col2, contrast = c(0.28, 0.87),
                                                 id = "COMUNE") + 
          
          tm_borders(alpha=.7) + tm_legend(legend.position = c("left", "bottom")) +
          tm_layout(title = paste(y,"medio per comune"),
                    title.size = 1.1) +
          
          tm_shape(OGR.reg) + tm_borders(col = "black") 
      }
      
      if(input$shape1 == "Sud" && input$all1==FALSE){
        
        tm <- tm_shape(OGR.com.sud) + tm_fill(input$red9, palette = input$palette9, style = "quantile", 
                                              n = input$g.col2, contrast = c(0.28, 0.87),
                                              id = "COMUNE") + 
          
          tm_borders(alpha=.7) + tm_legend(legend.position = c("left", "bottom")) +
          tm_layout(title = paste(y,"medio per comune"),
                    title.size = 1.1) +
          
          tm_shape(OGR.reg) + tm_borders(col = "black") 
      }
      
      if(input$all1==TRUE){
        tm <- tm_shape(OGR.com) + tm_fill(input$red9, palette = input$palette9, style = "quantile", 
                                              n = input$g.col2, contrast = c(0.28, 0.87),
                                              id = "COMUNE") + 
          
          tm_borders(alpha=.7) + tm_legend(legend.position = c("left", "bottom")) +
          tm_layout(title = paste(y,"medio per comune"),
                    title.size = 1.1) +
          
          tm_shape(OGR.reg) + tm_borders(col = "black") 
        
      }
      
      
      tmap_mode("view")
      
      tmap_leaflet(tm) 
      
    }) }) }
        
,options = list(height = 850))
  
    




```
<br><br>


## Spatial autocorrelation

Per autocorrelazione spaziale si intende quel fenomeno per cui nell’analisi spaziale di un dataset i valori di una certa variabile osservata saranno più simili in località contigue o vicine e diversi per località lontane tra loro. Questa caratteristica di una variabile che è autocorrelata viene comunemente associata alla disciplina dei processi stocastici, cioè processi che si evolvono nel tempo. In questo caso l’autocorrelazione è definita spaziale poiché ha effetto sullo “spazio” invece che sul tempo.

Oltretutto la prima legge della geografia, (Tobler 1970) dice che:

<br>
> “Ogni cosa è correlata a qualsiasi altra, ma le cose vicine sono più relazionate di quelle lontane”

<br>

L’autocorrelazione sarà quindi predominante quanto più la distanza si accorcia. 

Un’implicazione dell’autocorrelazione è il fatto che i dati spaziali non sono indipendenti, per questo si aprono ampi margini di studio di questa tipologia di dati. Lo sviluppo di strumenti analitici ha permesso di avere basi obiettive solide per decidere se in una determinata area geografica è presente un’ autocorrelazione significativamente valida.

La domanda che ci poniamo è quindi se il pattern oggetto di studio è frutto del caso o se ci sono influenze spaziali che hanno giocato un ruolo nella distribuzione spaziale della variabile.
I test per la presenza di autocorrelazione sono la base di partenza per la costruzione di modelli spaziali complessi, poiché senza questi rischieremmo di costruire modelli per poi scoprire che le strutture su cui questi modelli si basano sono del tutto insignificanti.

E’ bene ricordare che  l’autocorrelazione spaziale è stata sviluppata prendendo come riferimento dei punti collocati sullo spazio. Nel caso dello shape file utilizzato in questo lavoro le aree studiate (regioni,province,comuni) sono dei poligoni complessi ma la sostanza non cambia poiché l’analisi è estendibile ad oggetti più complessi dei punti, con le dovute accortenze.


### Struttura spaziale

Il primo passo in questo tipo di analisi è definire un metodo che ci permetta di incorporare la prossimità spaziale in una misura di autocorrelazione; A tal fine bisognerà catturare la relazione spaziale di ogni località e per fare ciò utilizzeremo una matrice di pesi spaziale $\textbf{W}$.
Nella prima riga di questa matrice troviamo l’informazione riguardante la prima località e tutte le altre. Formalmente ogni elemento della matrice $\textbf{W}$, definito come $w_{ij}$ rappresenterà la relazione tra la località i e la località j. Pertanto avremo:


$$
\begin{bmatrix} 
w_{11} & w_{12} & ... & w_{1n} \\
w_{21} & w_{22} & . & :\\
: & . & . & :\\
w_{n1} & ... & ... & w_{nn} \\
\end{bmatrix}
$$


Ogni valore della matrice è dipendente dalla relazione spaziale che lega le due località i e j e soprattutto dipende da come scegliamo di rappresentare questa relazione.

Considerato il framework bisogna assegnare un valore alle variabili $w_{ij}$.

L’approccio più semplice è quello dell’adiacenza. Con tale approccio assegneremo valore 1 se le due località sono adiacenti e valore 0 se non lo sono. Sebbene questo approccio sembri semplice esistono alcune varianti. Ad esempio potremmo considerare due poligoni come adiacenti se hanno un confine in comune (Rook’s case), o potremmo considerare sufficiente il fatto che i due poligoni condividano anche un solo vertice (Queen’s case).


Un’ altro approccio potrebbe essere quello di ignorare i confini dei poligoni e utilizzare una misura basata sulla distanza tra i centroidi dei poligoni. Basandoci su questo framework, dopo aver costruito la matrice delle distanze tra i centroidi potremmo definire una quantità arbitraria d e di conseguenza considerare adiacenti solo i poligoni le cui distanze sono minori del valore d.

Potremmo correttamente supporre, quindi, che la struttura della matrice $\textbf{W}$ descrive pienamente l'informazione sulla struttura spaziale. In letteratura molti studi sono stati proposti per descrivere le propietà di questa matrice, in particolare [@10.2307/621721] ha analizzato autovalori e autovettori di questa matrice identificando in questi ultimi l'informazione sulla struttura spaziale dei dati. In particolare gli autovalori della matrice forniscono una misura globale del livello di interconnessione del network, mentre gli autovettori indicano la centralità delle località rispetto all'intero sistema.  


Sebbene siano stati sviluppati molti metodi complessi per il problema di cui sopra ci limiteremo ad utilizzare i più semplici che verranno formalmente introdotti nel seguito.
Una considerazione importante da fare prima di procedere oltre riguarda la simmetria della matrice $\textbf{W}$. Saremmo portati a pensare che la matrice debba necessariamente essere simmetrica, e in linea di principio è così. Purtroppo alcuni dei metodi sviluppati per costruire questa matrice non garantiscono la simmetria. Possiamo forzare la matrice ad essere simmetrica mediante questa trasformazione:

$$
\begin{equation}
\textbf{W}_{final} = (1/2)(\textbf{W} + \textbf{W}^{T})
\end{equation}
$$

In ogni caso i metodi utilizzati nel corso di questo lavoro per la matrice $\textbf{W}$ garantiscono la simmetria, quindi non sarà necessario utilizzare la trasformazione sopra riportata.

Infine è necessario precisare che la definizione della matrice dei pesi è un passo fondamentale dell'analisi poichè le ipotesi che vengono fatte nel costruire questa matrice compongono il sistema di ipotesi del fenomeno studiato. E' infatti chiaro che qualora la matrice spaziale non rappresenti con fedeltà il sistema studiato i risultati dell'analisi potrebbero essere poco corretti. 


```{r}



shinyApp(
  
  ui <- fluidPage(
    
    sidebarLayout(
      
      # Sidebar panel per input ----
      sidebarPanel(
        
        selectInput(inputId = "shape10",
                    label = "Choose a macro to display",
                    choices = list("Nord",
                                   "Centro",
                                   "Sud"),
                    selected = "Nord"),
        
        
        selectInput(inputId = "method",
                    label = "Choose a method",
                    choices = list("Queen", "Distance"),
                    selected = ""),
        
        
        sliderInput(inputId = "distance",
                    label = "Choose distance",
                    value = 1,
                    min = 1.5,
                    max = 5,
                    step = 0.5),
        
        
        br(),
        
        
        actionButton("Run22", "Run code")
        
        
      ),
      
      
      mainPanel(
        
        # Output: Tabset w/ plot, summary, and table ----
        plotOutput("adj", width = "100%", height = 600)
        )
        
        
      )
    ),
  
  
  
  
  server = function(input,output) {
    
    output$adj <- renderPlot({
      

      if(input$Run22 == 0)
        return()
      isolate({
        
        OGR.prov.sub <- OGR.prov[OGR.prov@data$macro == input$shape10,]
        OGR.prov.sub@data$seq <- seq(1:length(OGR.prov.sub))
        xy.sub <- coordinates(OGR.prov.sub)
        
        ### Adicenze
        
        #Metodo semplice QUEEN

        wr.sub <- poly2nb(OGR.prov.sub, row.names = OGR.prov.sub$seq, queen = TRUE )
        
        
        
        #Metodo distance based
        if (input$method == "Distance"){
          
        dsts.sub <- unlist(nbdists(wr.sub,xy.sub))
        wr.sub <- dnearneigh(xy.sub, d1 = 0, d2 = input$distance * max(dsts.com),  
                             row.names = OGR.prov.sub@data$seq)
       
        }
        
        ## Plot adiacenze
        
        plot(OGR.prov.sub, col="gray", border="blue") # Disegna shapefile vuoto 
        plot(wr.sub, xy.sub, col="red", lwd="0.5", add=TRUE) #Disegna su shapefile precedente sia centroidi che collegamenti
        
 
      })

    })
        

  } ,
  
  options = list(height = 650)
  
)

```



## Moran's Index

Una volta determinata la struttura spaziale per l'analisi, possiamo procedere all'analisi dell'autocorrelazione spaziale. Nel corso del tempo sono stati sviluppati molteplici indici per valutare la presenza e la forza dell'autocorrelazione spaziale. 
La misura più utilizzata è l’indice di Moran. @10.1093/biomet/37.1-2.17 Il modo più semplice per presentare la misura è di immergersi direttamente nella sua formulazione analitica e valutarne ogni componente.
L’I di Moran è calcolato:


$$
\begin{equation}
\textbf{I} = \Bigg[ 
\frac{1}{\sum_{i=1}^{n} {(y_{i}-\bar{y})}^2 } 
\Bigg] 
\Bigg[  
\frac{\sum_{i=1}^{n}\sum_{j=1}^{n}w_{ij}(y_{i}-\bar{y})(y_{j}-\bar{y})}   
{\sum_{i=1}^{n}\sum_{j=1}^{n} w_{ij}} 
\Bigg]
\end{equation}
$$


La parte importante del calcolo è la seconda frazione. Il numeratore di questa frazione è


$$
\begin{equation}
\sum_{i=1}^{n}\sum_{j=1}^{n}w_{ij}(y_{i}-\bar{y})(y_{j}-\bar{y})
\end{equation}
$$


che si dovrebbe riconoscere come un termine di covarianza. I pedici $i,j$ si riferiscono a diverse unità spaziali e $y$ è il valore dei dati in ciascuna unità spaziale. Calcolando il prodotto delle differenze dalla media complessiva $\bar{y}$, determiniamo la misura in cui esse covariano. Se sia $y_{j}$ che $y_{i}$ si trovano sullo stesso lato della media (sopra o sotto di esso), allora questo prodotto è positivo; se uno è al di sopra della media e l'altro sotto, allora il prodotto è negativo e la dimensione assoluta del totale risultante dipenderà da quanto i valori sono vicini alla media complessiva.
I termini di covarianza sono moltiplicati per $w_{ij}$, cioè il corrispondente elemento della matrice dei pesi spaziali $\textbf{W}$, in questo modo gli elementi di covarianza sono ponderati in base alla loro relazione spaziale. Quando $\textbf{W}$ è una matrice binaria allora:

* Se le unità $i,j$ non sono adiacenti  $\Rightarrow w_{ij} = 0$

* Se le unità $i,j$ sono adiacenti  $\Rightarrow w_{ij} = 1$

In questo modo solamente i termini di covarianza relativi a due località adiacenti saranno inclusi nel calcolo.

Il denominatore della frazione, cioè la doppia sommatoria dei pesi, serve a tener conto della somma totale dei pesi e a relativizzare la misura calcolata al numeratore.

Il termine

$$
\begin{equation}
\frac{n}{\sum_{i=1}^{n} {(y_{i}-\bar{y})}^2 } 
\end{equation}
$$

non è altro che l'inverso della varianza dei dati, il che assicura che l’indice non sia grande semplicemente perché i valori e la variabilità in y sono elevati.


La conseguenza di una tale formulazione matematica dell'indice di Moran è che se i dati sono autocorrelati positivamente, allora la maggior parte delle coppie di posizioni adiacenti avrà valori sullo stesso lato della media e Moran avrà un valore positivo. D'altra parte, se i dati sono negativamente correlati , la maggior parte delle posizioni adiacenti avrà valori su lati opposti della media e il risultato complessivo sarà negativo.

Pertanto, per quanto riguarda un coefficiente di correlazione convenzionale, un valore positivo indica un'autocorrelazione positiva e un valore negativo una correlazione negativa. Il valore non è strettamente compreso nell'intervallo da 1 a -1, in quanto è impossibile che una mappa sia perfettamente autocorrelata, sia positivamente che negativamente, tranne che in situazioni molto insolite. In generale, un valore più o meno grande di [0.3;-0.3] indica autocorrelazione relativamente forte.

Per valutare la significatività statistica del valore ricavato verrà costruito un sistema di ipotesi e testato utilizzando una procedura bootstrap che verrà introdotta in seguito.



### Moran's scatterplot

Questo è un diagramma a dispersione Moran che mostra la relazione tra i valori degli attributi stessi (asse orizzontale) e il valore dell'attributo medio locale (ovvero il valore medio delle posizioni adiacenti). Questo grafico ha quattro regioni: il quadrante in basso a sinistra contiene casi in cui il valore dell'attributo in ciascun poligono e il valore dell'attributo medio dei poligoni vicini sono inferiori alla media globale; il quadrante in alto a destra contiene casi in cui sia il valore dell'attributo che la media locale sono maggiori della media globale; e gli altri due quadranti contengono casi in cui il valore dell'attributo e la media locale si trovano su lati opposti della media globale. Le posizioni che si trovano nei quadranti in basso a sinistra e in alto a destra sono quelle che contribuiscono all'autocorrelazione positiva complessiva, poiché hanno un valore di attributo simile a quello dei loro vicini, mentre le posizioni negli altri due quadranti contribuiscono a un'autocorrelazione negativa. Se la maggior parte delle posizioni si trova nei quadranti in basso a sinistra e in alto a destra, allora il risultato sarà probabilmente un valore positivo dell'I di Moran, cioè un'autocorrelazione positiva.


```{r}

Moran.fun <- function(shape,x,wm){
    
    n <- length(shape)
    y <- shape[[etichette[x]]]
    ybar <- mean(y, na.rm = TRUE)
    
    # Ora ci serve (yi - ybar)(yj - ybar)
    
    dy <- y - ybar # Scarto dalla media (vettore)
    g <- expand.grid(dy, dy) # Combinazione del vettore
    yiyj <- g[,1] * g[,2]
    
    pm <- matrix(yiyj, ncol = n) # Crea matrice da yiyj
    
    pmw <- pm * wm #pm * matrice dei pesi -> (yi - ybar)(yj - ybar)*wij
    spwm <- sum(pmw) #Doppia sommatoria di -> (yi - ybar)(yj-ybar)*wij
    smw <- sum(wm) #Somma dei pesi
    sw <- spwm/smw #Doppia sommatoria "spwm" diviso somma dei pesi "smw"
    vr <- n / sum(dy^2) # Prima parte della formula
    
    # Varianza
    
    S0 <- smw # Somma dei pesi
    
    a1 <- wm.prov + t(wm.prov) 
    a2 <- rowSums(a1)^2
    S2 <- sum(a2)
    
    b1 <- a1^2
    S1 <- sum(b1)
    
    A <- n*((n^2-3*n+3)*S1-n*S2+3*S0^2)
    D <- sum(dy^4)/(sum(dy^2))^2
    B <- D*((n^2-n)*S1-2*n*S2+6*S0^2)
    C <- (n-1)*(n-2)*(n-3)*S0^2
    
    m.sec <- (A-B)/C
    
    
    
    EI <- -1 /(n-1) # Valore atteso teorico sotto H0 cioè assenza di correlazione spaziale
    MI <- vr * sw # Calcolo moran index
    VI <- m.sec-EI^2
    
    
    
    #print(paste("Moran I",round(MI, digits = 7)))
    #print(paste("Expected",round(EI, digits = 7)))
    
    
    
    
    invisible(base::list(
      "Moran" = MI,
      "Expected" = EI,
      "Variance" = VI,
      "Shape" = deparse(substitute(shape)),
      "Variable" = etichette[x],
      "Weight" = deparse(substitute(wm))))
    
    #E' come return ma non printa ulteriormente il valore
    
    
  } #E' necessario chiamare funzione anche qui altrimenti ci sono problemi di environment

shinyApp(

        
ui <- fluidPage(

  sidebarLayout(

    # Sidebar panel per input ----
    sidebarPanel(
      
                        selectInput(inputId = "red5",
                              label = "Choose a variable to display",
                              choices = etichette,
                              selected = ""),

                        actionButton("Run5", "Run code")

    ),

    mainPanel(

            mainPanel(plotlyOutput("moran", width =500, height = 500),
                      verbatimTextOutput("moran.text"))
            
    )
  )
),



  server <- function(input,output) {
          
      
  output$moran <- renderPlotly({
    
      if(input$Run5 == 0)
        return()
      isolate({
      
             n <- length(OGR.prov)
             z <- input$red5
             y <- OGR.prov@data[[z]]
             ybar <- mean(y)
    

             ms <- cbind(id = rep(1:n, each = n), y = rep(y, each = n),
                         value = as.vector(wm.prov * y))
             ms <- ms[ms[,3] > 0,]
             ams <- aggregate(ms[,2:3], list(ms[,1]), FUN = mean)
             ams <- ams[,-1]
             head(ams)
             
             p <- ggplot(data = ams, aes(x=y, y= value)) + 
               geom_point() + geom_smooth(method = "lm", color = "black") +
               geom_hline(yintercept=mean(ams[,2]), linetype="dashed", color = "red") +
               geom_vline(xintercept = ybar, linetype="dashed", color = "red")
             
             #p <- plotly(p)
             ggplotly(p)
             
    
        }) })
    
  
  output$moran.text <- renderPrint({
      
      if(input$Run5 == 0)
        return()
      isolate({
      
      n <- length(OGR.prov)
      z <- input$red5
      y <- OGR.prov@data[[z]]
      ybar <- mean(y)

      Moran.I <- Moran.fun(OGR.prov, which(etichette == z), wm.prov)
      
      print.moran(Moran.I)

    }) })
    
  
    } ,
  
  options = list(height = 800)
  
)

```




Vale la pena notare che l'I di Moran è effettivamente il coefficiente di correlazione per la relazione tra i valori degli attributi e i valori degli attributi medi locali. L 'equazione per l'I di Moran viene riscritta in forma di matrice come: 


$$
\begin{equation}
\textbf{I} = \Bigg[ 
\frac{n}{\sum_{i=1}^{n}\sum_{j=1}^{n} w_{ij} } 
\Bigg] * 
\frac{\textbf{y}^T\textbf{W}\textbf{y}}
{\textbf{y}^T\textbf{y}} 
\end{equation}
$$


ove y è il vettore della colonna le cui voci sono ciascuna (y i – y) $(y_i - y)$.
Questa intuizione consente di utilizzare le statistiche diagnostiche standard della regressione lineare per associare i p-value ai valori osservati di I di Moran. Tuttavia, poiché la struttura spaziale della mappa è anche un parametro nell'analisi, un approccio più usuale si basa su una procedura Monte Carlo.
I valori degli attributi di posizione possono essere permutati per qualsiasi numero richiesto di volte , ovvero i valori degli attributi osservati nella mappa vengono assegnati in modo casuale alle posizioni della mappa e l'I di Moran viene ricalcolato ogni volta per la mappa generata dall'assegnazione casuale. Ciò fornisce una distribuzione empirica di campionamento per l'indice e consente all'I Moran osservato di essere valutato in termini di inusualità rispetto a questo benchmark randomizzato.



## Bootstrapping Moran's 

$$
\begin{equation}
E[I^{2}] = \frac{A-B}{C} \\
A = n\Big[(n^{2}-3n+3)S_{1} - nS_{2}+3S^{2}_{0}\Big] \\
B = D\Big[(n^{2}-n)S_{1}-2nS_{2}+6S^{2}_{0}\Big] \\
C = (n-1)(n-2)(n-3)S^{2}_{0} \\

D = \frac{\sum_{i=1}^{n} z^{4}_{i} }
{\Big(\sum_{i=1}^{n} z^{2}_{i} \Big)^{2}} \\

S_{0} = \sum_{i=1}^{n}\sum_{j=1}^{n}  w_{i,j}   \\

S_{1} = (1/2)\sum_{i=1}^{n}\sum_{j=1}^{n} \big(w_{i,j} + w_{j,i}\big)^{2} \\

S_{2} = \sum_{i=1}^{n} \Big( 
\sum_{j=1}^{n} w_{i,j} + \sum_{j=1}^{n} w_{j,i}
\Big)^{2}
\end{equation}
$$


```{r}
#E' necessario chiamare funzione anche qui altrimenti ci sono problemi di environment
Moran.fun <- function(shape,x,wm){
    
    n <- length(shape)
    y <- shape[[etichette[x]]]
    ybar <- mean(y, na.rm = TRUE)
    
    # Ora ci serve (yi - ybar)(yj - ybar)
    
    dy <- y - ybar # Scarto dalla media (vettore)
    g <- expand.grid(dy, dy) # Combinazione del vettore
    yiyj <- g[,1] * g[,2]
    
    pm <- matrix(yiyj, ncol = n) # Crea matrice da yiyj
    
    pmw <- pm * wm #pm * matrice dei pesi -> (yi - ybar)(yj - ybar)*wij
    spwm <- sum(pmw) #Doppia sommatoria di -> (yi - ybar)(yj-ybar)*wij
    smw <- sum(wm) #Somma dei pesi
    sw <- spwm/smw #Doppia sommatoria "spwm" diviso somma dei pesi "smw"
    vr <- n / sum(dy^2) # Prima parte della formula
    
    # Varianza
    
    S0 <- smw # Somma dei pesi
    
    a1 <- wm.prov + t(wm.prov) 
    a2 <- rowSums(a1)^2
    S2 <- sum(a2)
    
    b1 <- a1^2
    S1 <- sum(b1)
    
    A <- n*((n^2-3*n+3)*S1-n*S2+3*S0^2)
    D <- sum(dy^4)/(sum(dy^2))^2
    B <- D*((n^2-n)*S1-2*n*S2+6*S0^2)
    C <- (n-1)*(n-2)*(n-3)*S0^2
    
    m.sec <- (A-B)/C
    
    
    
    EI <- -1 /(n-1) # Valore atteso teorico sotto H0 cioè assenza di correlazione spaziale
    MI <- vr * sw # Calcolo moran index
    VI <- m.sec-EI^2
    
    
    
    #print(paste("Moran I",round(MI, digits = 7)))
    #print(paste("Expected",round(EI, digits = 7)))
    
    
    
    
    invisible(base::list(
      "Moran" = MI,
      "Expected" = EI,
      "Variance" = VI,
      "Shape" = deparse(substitute(shape)),
      "Variable" = etichette[x],
      "Weight" = deparse(substitute(wm))))
    
    #E' come return ma non printa ulteriormente il valore
    
    
} 
  
Moran.boot <- function(shape,x,wm,n.sim){
    
    
    Morans <- NULL
    
    m.list <- Moran.fun(shape,x,wm) #, print.boot = TRUE)
    
    for (i in 1:n.sim){
      
      w.per <- wm[sample(nrow(wm)),sample(ncol(wm))]
      
      Morans[i] <- Moran.fun(shape,x,w.per)[[1]] 
      
    }
    
    MI <- m.list[[1]]
    EI <- m.list[[2]]
    VI <- m.list[[3]]
    ZS <- (MI - EI)/(sqrt(VI))
    p.value <- 2*pnorm(-abs(ZS)) 
    
    
    invisible(base::list(
      "Moran" = MI,
      "Expected" = EI,
      "Variance" = VI,
      "Shape" = deparse(substitute(shape)),
      "Variable" = etichette[x],
      "Weight" = deparse(substitute(wm)),
      "N.sim" = n.sim,
      "Simulated" = as.vector(Morans),
      "z-score" = ZS,
      "p.value" = p.value))
    
  }

print.moran <- function(moran.list, boot = FALSE, plot = FALSE) {
    
    if(boot == FALSE){
      
      cat("\n")
      cat("                         Moran's I  \n")
      cat("Shape: ", moran.list[[4]], "\n") # deparse(substitute(x)) serve a far leggere il nome dell'oggetto nell'environment
      cat("Data: ", moran.list[[5]], "\n")                 
      cat("Weights: ", moran.list[[6]], "\n") # senza deparse(sub(..)) e con wm solo,  avremmo avuto una stringa di 01011010001 poichè mostrava gli elementi
      cat("\n")
      cat("Moran I statistic: ", moran.list[[1]], "\n") # cat fa un paste() degli argomenti in c("..","..","...")
      cat("Expected value: ", moran.list[[2]], "\n") # \n significa a capo
      cat("Variance: ", moran.list[[3]], "\n") 
      
    } else {
      
      
      cat("\n")
      cat("           Moran I test under randomization      \n")
      cat("Shape: ", moran.list[[4]], "\n") # deparse(substitute(x)) serve a far leggere il nome dell'oggetto nell'environment
      cat("Data: ", moran.list[[5]], "\n")                 
      cat("Weights: ", moran.list[[6]], "\n") # senza deparse(sub(..)) e con wm solo,  avremmo avuto una stringa di 01011010001 poichè mostrava gli elementi
      cat("N.sim: ", moran.list[[7]], "\n")
      cat("\n")
      cat("Moran I statistic: ", moran.list[[1]], "\n") # cat fa un paste() degli argomenti in c("..","..","...")
      cat("Expected value: ", moran.list[[2]], "\n") # \n significa a capo
      cat("Variance: ", moran.list[[3]], "\n")
      cat("Z-score: ", moran.list[[9]], "    p-value: ", moran.list[[10]], "\n")
      
      if(plot == TRUE){
        a <- as.data.frame(moran.list[8])
        
        p <- ggplot(a, aes(x = Simulated)) + 
          geom_histogram(color = "black", fill = "white", binwidth=0.003) +
          geom_vline(aes(xintercept = moran.list[[1]]), color = "red", linetype = "dashed", size = 1)
        
        p
      }
      
    }
    
  }
  

shinyApp(

        
ui <- fluidPage(

  sidebarLayout(

    # Sidebar panel per input ----
    sidebarPanel(
      
                selectInput(inputId = "red6",
                            label = "Choose a variable to display",
                            choices = etichette,
                            selected = ""),
              
              
              numericInput(inputId = "n.sim",
                           label = ("Choose n iteration"),
                           value = "20"),
              
              actionButton("Run6", "Run code")

    ),

    mainPanel(

            mainPanel(plotlyOutput("bootstrap", width = 550, height = 600),
                      div(style = "width:550px", verbatimTextOutput("bootstrap.text")))
            
    )
  )
),



  server <- function(input,output) {
          
  output$bootstrap <- renderPlotly({
      
      
      if(input$Run6 == 0)
        return()
      isolate({
        
 
             z <- input$red6
             zz <- input$n.sim
             
             
          
             M.boot <- Moran.boot(OGR.prov, which(etichette == z), wm.prov, zz)
             
             print.moran(M.boot, boot = TRUE, plot = TRUE)
            
             
             # a <- cbind(OGR.prov[[z]],rep(mean(OGR.prov[[z]]), n= length(OGR.prov)))
             # a <- as.data.frame(a)
             # names(a) <- c("Observed","Expected")
             # ww <- nb2listw(wr.prov, zero.policy = TRUE)
             # niter <- zz
             # 
             # moran.mboot<-boot(a, statistic=moranI.pboot, sim="parametric",
             #                   ran.gen=poisson.sim,
             #                   R=niter, listw=ww.prov,
             #                   n=length(wr.prov),
             #                  S0=Szero(ww.prov) )
             # plot(moran.mboot)
    
    }) })

      
  output$bootstrap.text <- renderPrint({
      
      
      if(input$Run6 == 0)
        return()
      isolate({
        
        
        z <- input$red6
        zz <- input$n.sim
        
        
        
        M.boot <- Moran.boot(OGR.prov, which(etichette == z), wm.prov, zz)
        
        print.moran(M.boot, boot = TRUE)
        
        
        
        
        # a <- cbind(OGR.prov[[z]],rep(mean(OGR.prov[[z]]), n= length(OGR.prov)))
        # a <- as.data.frame(a)
        # names(a) <- c("Observed","Expected")
        # ww <- nb2listw(wr.prov, zero.policy = TRUE)
        # niter <- zz
        # 
        # moran.mboot<-boot(a, statistic=moranI.pboot, sim="parametric",
        #                   ran.gen=poisson.sim,  R=niter, listw=ww.prov,
        #                   n=length(wr.prov),
        #                   S0=Szero(ww.prov) )
        # print(moran.mboot)
        
      }) })
   

  
    } ,
  
  options = list(height = 860)
  
)

```






